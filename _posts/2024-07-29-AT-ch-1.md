---
layout: post
title: "Chapter 1: Backtesting and Automated Execution"
tags: [Algorithmic Trading By Ernest Chan]
---
## Chapter 1: Backtesting and Automated Execution
**Disclaimer:** Filled with god-awful typos / nasty run-ons / incomplete sentences! For personal use, proceed with caution! 

Source Material
* [Algorithmic Trading: Winning Strategies and Their Rationale](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146)

---
### The importance of backtesting

Definition: Backtesting
Backtesting is the process of feeding historical data to your trading strategy to see how it would perform. 

We use backtesting to not only test our own strategies but also test others and pinpoint details missed by researchers. Ideally, we would implement these into our own automated execution by transforming our backtesting system. 

Backtesting a published strategy allows you to conduct true out-of-sample testing in the period following the publication without data warping. 

Once we implement every detail of a strategy as a backtesting program, we can put them through a microscope and look for pitfalls in the backtesting process or in the strategy itself. 

### Common pitfalls of backtesting 

### **Look-ahead bias**
Definition: Look ahead bias 
This bias means you backtest using future prices to determine today's trading signals. More generally, future information is testing data. 
- programming error that can infect backtest program and live trading program
- if backtesting and live trading programs are one of the same, only difference is the data fed in (historical vs live) there can be no look-ahead bias in the program 

entry signal: indicator for optimal entry of trade 

### Data snooping bias and beauty of linearity 

**Data snooping:**
caused by having too many free parameters that are fitted to random ethereal market patterns in the past to make historical performance look good 
- random market patterns unlikely to reoccur in future
- model fit to these patterns is unlikely to have much predictive power
- Mystical Overfitting

**To detect data snooping:**
Test the model on out-of-sample data and reject the model that doesn't pass the tests. Tweaking the out-of-sample data turns it into in-sample data. Don't be a dweeb and do that. 

**Cross validation:**
Instead of tweaking the model and using cherry picked data, use cross validation. Select a number of different subsets of data for training and tweak the model accordingly.
- make sure the model performs well on the different subsets

We prefer models with high sharpe ratios and short max drawdown durations is that it almost automatically ensures the model will pass the cross-validation test: only subsets where the model will fail the test are those with rare downturn periods. 

**sharpe ratio:**
Risk-adjusted return 
$$\text{Sharpe Ratio} = \frac{R_i - R_f}{\sigma_i}$$
- $R_i$: return of investment
- $R_f$: risk free rate
- $\sigma_i$: std. dev of returns (volatility)


**drawdown duration:** 
Reduction of value from peak to trough 
$$\text{Drawdown} = \frac{\text{Peak Value} - \text{Trough value}}{\text{Peak Value}}$$

**General approach**:
Make the model as simple as possible can minimize snooping boas. A model with few parameters but lots of complicated trading rules is just as susceptible to bias (as well as non-linear models).  

Example: If we want to predict price by simple extrapolation of the historical price series, a nonlinear model would fit the historical data better but there is no guarantee it can predict a future value better. Just because the number of parameters are the same for both types of models, they are not the same. 

Kerotsis:
measure if data is heavy-tailed or light-tailed relative to normal dist. 

pareto distribution: 
80/20 rule states that 80% of outcomes are due to 20% of causes. 

**Occam's razor:**
Philosophical problem-solving principle that recommends searching for explanations constructed with the smallest possible set of elements. 

Occam's razor dictates that unless there are strong theoretical and empirical reasons to suppose non-gaussian distributions, the Gaussian form should be assumed. 

Linear formulas $\rightarrow$ linear capital allocation formula


**Capital allocation formula**:
CAL is a graph created by investors to measure the risk of risky and risk-free assets. Its slope is also known as the *reward-to-variability ratio*. 

$$CAL: E(r_c) = r_F + \sigma_C\frac{E(r_P)-r_F}{\sigma_P}$$
P is the risk portfolio, F is the riskless portfolio and C is the combination. 
Interpretation: incremental return of the portfolio to the incremental increase of risk. 


**Profits are not derived from some subtle complicated cleverness of the strategy but from the intrinsic inefficiency in the market that is hidden in plain sight**. 

The most extreme form of linear predictive models is one in which all the coefficients are equal in magnitude (but not necessarily in sign). If we normalize these factors, but turning them into Z scores
$$\tag{1.1} z(i) = (f(i) - mean(f))/std(f))$$

we can predict tomorrow's return $R$ by,
$$\tag{1.2} R = mean(R) - std(R)\sum^n_{i}sign(i)z(i)/n$$

$sign(i)$ is the historical correlation between $f(i)$ and $R$. Mean and std of $R$ correlate to one-day returns and $f$ correlate to various $i$ factors. WQual weighting to all predictors are often superior because they are not affected by accidents of sampling. 

*relative* returns vs *absolute* returns is better. If we use this to rank stocks, and then form a long-sort portfolio by buying stocks in the top decile and shorting in the bottom decile, the average return of the portfolio is often positive. 


If our goal is to jut rank stocks:
combine $f$'s by using rank,
$$\tag{1.3} rank_s = \sum^n_isign(i)rank_s(i)$$

Greenblatt two factor model:
$f(1)$: return on capital
$f(2)$: earnings yield 

No matter how carefully you try to prevent data snooping, it will creep into your model so use *walk-forward testing*.

**Walk forward testing**: 
Used to determine optimal parameters for a trading strat and to determine the robustness of the strat. 
- optimized with in-sample data for a time window in a data series
-  the remaining data is reserved for out of sample testing
-  small portion of the reserved data following the in sample data is tested and the results are recorded
-  time period shifted forward, process repeated

Also called paper trading because we're virtually trading (not using real money) on live markets. 

### Stock splits and dividend adjustments 

Whenever a company has a stock split of $n-1$ split, the stock price will be divided $n$ times. In the backtest, we typically look at just the price series to determine our trading signals, not the market value series of some account. So unless we back adjusted the prices before the ex-date of the split, we will see a sudden drop in price that might trigger wrong trading signals. 

In the same vein, when a company pays cash dividends of $d$ amount, the stock amount will also go down some $d$ amount. That is because if you own that stock before the dividend ex-date, you will get cash (or stock) distributions in your brokerage account so there should be no change in market value. But if not back-adjusted, can lead to trading signal errors. 

Note: price will decrease because (thx u chatty)
- Value Transfer: When a company pays a dividend, it distributes part of its earnings to shareholders. The value of this cash is subtracted from the company's total value. As a result, the company's market capitalization decreases, leading to a proportional drop in its stock price.

- Ex-Dividend Date: The stock price typically drops by the amount of the dividend on the ex-dividend date. This is because new buyers of the stock on or after this date are not entitled to receive the declared dividend. The drop reflects the reduction in value since the dividend has been effectively detached from the stock.

- Market Perception: Sometimes, paying dividends can signal that a company lacks better investment opportunities for its retained earnings. This perception can cause a decrease in the stock price as investors might prefer companies that reinvest earnings into growth opportunities.

  
### **Survivorship bias in stock database**
If you are backtesting a stock-trading model you will suffer from survivorship bias if your historical data does not include delisted stocks. 

Extreme case example: If your model asks you to buy the one stock that dropped the most in the previous dat and hold it forever, it will perform poorly because in many cases the company performing the worst will go bankrupt resulting in the 100% loss of the position. 

Survivorship bias is more dangerous to mena-reverting long-only stock strategies than to mean reverting long-short or short-only strategies. This is because this bias tends to *inflate the backtest performance of a long-only strat* that buys low and sells high. It would *deflate* the backtest performance of a short-only strat that first sells high and then buys low. 

Inflation of the long strategy return tends to outweigh the deflation of the short portfolio return. 

### **Primary vs. consolidated stock prices**
When you submit a market on close (MOC) or a market on open (MOO) order, it will always be routed to the primary exchange only. 

**MOC** Market-on-Close (MOC) Order: An order to buy or sell a security at the market price as close as possible to the market's closing price. 

**MOO** Market-on-Open (MOO) Order: An order to buy or sell a security at the market price at the opening of the trading day. 

If you have a strategy that relies on MOO or MOC orders, you need the historical prices from the primary exchange to accurately backtest your model. The transaction prices on the next trading day will usually mean revert from hard to achieve outlier prices. 

Similar considerations apply to using high or low prices for strategies. Historical data is usually consolidated highs or lows, not that of the primary exchange. They are often unrepresentative, exaggerated numbers resulting from trades of small sizes on secondary markets.

### **Venue dependence of currency quotes**
Compared to the stock market the currency markets are even more fragmented and there is no rule that says a trade executed at one venue has to be at the best bid or ask across all different venues. 

*A backtest will only be realistic if we use historical data extracted from the same venue(s) as the one(s) we expect to trade on.*
- trade prices and sizes are not generally available, but we can use bid-ask quotes for backtesting forex strategies since the spreads for the same currency pair can vary significantly between venues.

### **Short sale constraints** 
A shorting strategy assumes that the stocks can be shorted, but there are often difficulties in shorting: 
- the broker needs to locate a quantity of their stocks from other customers and institutions
- stock can be hard to borrow if large short interest is out there so there are a lot of shares of a company that have already been borrowed or the float of the stock is limited
- we need to pay interest to the stock lender

**Stock float** A stock float, also known as floating stock, is the number of shares of a company's stock that are available for public investors to buy and sell on financial exchanges and stock markets.

Backtesting on data where the stocks are hard to borrow, the results might look too good. It's hard to find a historically accurate list of hard-to-borrow stocks for backtest. 

General rule: *small-cap stocks are affected much more by short sale constraints than are large-cap stocks*
- returns of their short position are much more suspect.

*Uptick rule*
- short sale had to be executed at a price higher than the last traded price, or at the last traded price if that price was higher than the price of the trade prior to the last (NASDAQ price needs to be higher than the last bid).
*Alternative uptick rule*
-  short sale to have a trade price higher than the national best bid but only when a circuit breaker has been triggered.

**circuit breaker**: Circuit breaker is a regulatory mechanism used by stock exchanges to temporarily halt trading on an exchange to curb panic-selling and excessive volatility

The backtest will be inflated if these factors are not taken into account.

### **Futures Continous contracts** 

Futures contracts have expiration dates so a trading strategy on crude oil futures is really a trading strategy on many different contracts. Which contract is the front month is when you plan to roll over to the next month, or when you plan to sell the current front contract and buy the contract with the nearest expiration date. 

*front month contracts*: Front month contracts, also known as "near-month" contracts, refer to the futures contracts that are closest to expiration.

Someone might decide to roll over 10 days before the current front contract expires and others may decide to roll over when there is an *open interest crossover* - when the open interest of the next contract exceeds that of the current front contract. 

**open interest crossover**: Open interest crossover refers to a technical analysis concept in futures and options trading, where the open interest (the total number of outstanding contracts) of one contract month crosses above or below the open interest of another contract month. 

**Creating a continuous contract** 
1. concatenate the prices of the front-month contract together given a set of rollover dates
     - this results in big price gaps from rollover date to the next, create a false return or P&L on the rollover date in your backtest
2. 
     - front contract on date T: $p(T)$
     - closing price of same contract on date $T+1$: $p(T+1)$
     - clsing price of nearby contract on date $T+1$: $q(T+1)$
     - $T+1$: rollover date
  suppose we are long the front contract, we should sell this contract at the close at $p(T+1)$ and then buy the next contract at $q(T+1)$. What's the P&L on $T+1$?

P&L is $p(T+1)-p(T)$ and the return is $\frac{p(T+1)-p(T)}{p(T)}$. But the unadjusted continuous price series will show a price of $p(T)$ at $T$, and $q(T+1)$ at $T+1$. 
Contract Rollover: When rolling over from the front contract to the back contract, there's typically a price difference between these contracts. This difference can be due to factors such as carrying costs, storage costs, interest rates, and market expectations about future price movements.

Unadjusted Price Series: If the price series data is not adjusted for the rollover, it will show a sudden jump or gap between the price of the front contract on the last trading day (T) and the price of the back contract on the first trading day post-rollover (T+1).

If you don't adjust for the rollover, you end up using the price of the back contract after the rollover date, which can lead to misleading calculations for P&L and returns.

We would then calculate erroneous terms of profit and return as: $q(T+1)-p(T)$, $\frac{q(T+1)-p(T)}{p(T)}$. To prevent this error back adjust the data series to eliminate the price gap so that P&L on $T+1$  is $p(T+1)$ -p(T)$.
- **Return back adjustment**: we can add $q(T+1)$ - p(T+1)$ to every price $p(t)$ but our problem is still not solved as return calculation will be incorrect 
- **Price back adjustment**: you can back-adjust the price series to make the return calculation correct by multiplying every price p(t) on every date $t$ on or before $T$ by the number $q(T+1)/p(T+1)$ but the P&L will be incorrect

Trade-off between correct P&L or return, we have to choose one performance measure. 
when choosing the price back adjustment instead of return, the prices may turn negative in the distant past. This may create problems for the trading strategy and will create problems in calculating returns. We can transform the pricing with a universal constant so none will be negative. 

Picking the right back adjustment method is more important when we have a strat that involves trading spreads between different contracts. If your strat generates trading signals based on the price difference between two contracts, then you must choose the price back-adjustment method otherwise the price difference may be wrong and generate a wrong trading signal. 

If you are using a calendar spread then a back adjustment is even more important. A calendar spread is a small number compared to the price of one left of the spread, so any error due to rollover will be a significant percentage of the spread and likely to trigger a wrong signal both in backtest and in live trading. 

### **Futures close vs. settlement prices**

The daily closing price of a future contract provided by a daily vendor is usually the settlement price, not the last traded price of the contract during the day. Note that a futures contract will have a settlement price each day (determined by exchange) even if the contract has not been traded at all that day. 

If the contract is traded, the settlement price is in general different from the last traded price. Most historical data vendors provide the settlement price as the daily closing price. But some provide tick-by-tick data and actual transaction price only, and therefore the close price will be the last trade price if there has been a transaction on that day. 

*In most cases we should use the settlement price, because if you had traded live near the close, that would have been closest to the price of your transaction*. 
- the last recorded trade price might have occurred several hours earlier and bear little relation to your transaction price near the close
- if we're constructing a pair trading strategy on futures this is especially important!!
- If you use the settlement prices to determine the futures spreads, 
you are guaranteed to be using two contemporaneous prices.

If you use the last traded prices to determine the spread, you may be using prices generated at two very different times and therefore be incorrect. 

**intraday spread strategy**: An intraday spread strategy involves trading the price difference between two related futures contracts within the same trading day

If you have a *intraday spread strategy* or using intraday futures prices for backtesting a spread strategy, you will need either historical data with bid and ask prices of both contracts of the intraday data on the spread itself when it is native to the exchange. 
- many futures are not very liquid so if we use the last price of every bar to form a spread, we may find that the last prices of contract A and B of the same bar may actually refer to transactions that are quite far apart in time.


A spread formed by asynchronous last prices could not in reality be bought or sold at those prices. Backtests of intraday spread strategies using the last price of each leg of the spread instead of the last price of the spread itself will again inflate the resulting returns. 

**Last Price of Each Leg:** Refers to the individual closing prices of each contract in the spread.

If contracts are traded on different exchanges they are likely to have different closing times. So it would be wrong to form an Intermarket spread using their closing prices. 
- this is also true if we try to form a spread between a future and an ETF
- we can instead obtain a intraday bid-ask data so that synchronicity is assured. The other possibility is to trade an ETF that holds a future instead of just a future itself.

### **Statistical significance of backtesting: hypothesis testing**

We face the problem of finite sample sizes and because of this randomness. We use hypothesis testing to address this issue

The basic framework is as follows:
1. based on a backtest on some finite sample of data, we compute a certain statistical measure called the test statistic
    -  (ex: average daily return of a trading strategy in that period)
2. we suppose that the true avg. daily return based on infinite data is actually 0 (null hypothesis)
3. probability distribution based on data is known and has a zero mean based on the null hypothesis.
4.  based on null hypothesis probability distribution, we compute the probability $p$ that the average daily returns will be at least as large as the observed value in the backtest
    - this probability $p$ is called the p-value and it's very small (0.01) therefore we can reject the null hypothesis and conclude that the backtested average daily return is statistically significant 

*How do we determine the probability measure under the null hypothesis?* 
- if the Gaussian distribution centered around 0, then a high sharpe ratio allows us to easily reject the null hypothesis. The z-stat for Gaussian is $\frac{\bar{x}-\mu}{\sigma / \sqrt{n}}$

This is consistent with our belief that high sharpe rations are more statistically significant. 
We can also estimate the probability distribution of the null hypothesis using *Monte carlo methods*. 
- can be used to simulate historical price data and feed these simulated data into our strategy to determine the empirical probability distribution of profits
- we believe that the profitability of the trading strategy captured some subtle patterns or correlations of the price series and not just because of the first few moments of the price distortions

- generate many simulated price series with the same first moments and the same length as the actual price data, we can find out what fraction $p$ of these price series are the average returns greater than or equal to backtest returns.


We can also use a generated set of simulated trades with the constraint that the number of short and long entry trades is the same as the backtest and the same average holding periods for the trades. These trades are distributed randomly over the actual historical price series. We then measured what fraction of such sets of trades has an average return greater than or equal to the backtest average return. 

### Example 1.1: Hypothesis testing on future momentum strategy
Second hypothesis test:
*If we find that there is a good probability that the strategy can generate an as good as or better return on this random returns series as the observed return series, it would mean that the momentum strategy is not actually capturing the momentum*

---
**Note** Pearsonr fucntion not transferable in python

<details open> 
  <summary>Code: T2 Simulated random returns with the prescribed moment</summary>
  
```python
  # define market returns (replace with actual data)
  marketRet = np.array([np.random.randn(1000)]) # np.randn(1000) for demonstration 
  
  # complete moments
  mean_ret = np.mean(marketRet)
  std_ret = np.std(marketRet)
  skewness_ret = skew(marketRet)
  kurtosis_ret = kurtosis(marketRet)
  moments = [mean_ret, std_ret, skewness_ret, kurtosis_ret]
  
  # simulation parameters
  num_samples = 10000
  lookback = 20
  holddays = 5
  numSampleAvgretBetterOrEqualObserved = 0
  
  def backshift(n, series):
      """Shift the series by n positions, wrapping around."""
      return np.roll(series, n)
  
  def pearsonrnd(moments, size):
      """Generate random data based on the given moments. 
      This is a placeholder, pearsonrnd function package not translated into python."""
      mean, std, kewness, kurt = moments
      return np.random.normal(mean, std, size)

  # simulate and evaluate
  for _ in range(num_samples):
      marketRet_sim = pearsonrnd(moments, len(marketRet))
      cl_sim = np.cumprod(1 + marketRet_sim) - 1
      longs_sim = cl_sim > backshift(lookback, cl_sim)
      shorts_sim = cl_sim < backshift(lookback, cl_sim)
  
      pos_sim = np.zeros(len(cl_sim), dtype=int)
      for h in range(holddays):
          long_sim_lag = backshift(h, longs_sim)
          long_sim_lag[np.isnan(long_sim_lag)] = False
          long_sim_lag = long_sim_lag.astype(bool)
          short_sim_lag = backshift(h, shorts_sim)
          short_sim_lag[np.isnan(short_sim_lag)] = False
          short_sim_lag = short_sim_lag.astype(bool)
          
          pos_sim[long_sim_lag] += 1
          pos_sim[sort_sim_lag] -= 1
  
      ret_sim = backshift(1, pos_sim) * marketRet_sim / holddays
      ret_sim[np.isnan(ret_sim) | np.isinf(ret_sim)] = 0
  
      if np.mean(ret_sim) >= np.mean(marketRet):
          numSampleAvgretBetterOrEqualObserved += 1
```
</details>

<details> 
  <summary>Pseudocode: Simulated random returns with the prescribed moment</summary>

```python
  """ 
  1. define market returns data (marketRet)
  ---
  2. calc statistical moments of market returns:
  - mean of marketRet
  - std dev marketRet
  - skewness of marketRet
  - kurtosis of marktRet
  - store into list (moments)
  ---
  3. set simualtion param:
  - num_samples = 10000 (num sim)
  - lookback = 20 (lookback period)
  - holddays = 5 (holding day for position)
  - init counter for samples to 0 (numSampleAvgretBetterOrEqualObserved)
  ---
  4. define backshift function:
    - Input: num positions (n) time series (series)
    - output: time series shifted by n positions with wrapping
  ---
  5. define pearsonrnd function
  - (not included)
  ---
  6. for each sample from 1 to num_samples:
  - generate sim market returns (marketRet_sim) using pearsonrnd
  - calc cum returns (cl_sim) from marketRet_sim
  - determine long positions (longs_sim) where cl_sim > lookback period
  - determine short positions (shorts_sim) where cl_sim < lookback period
  
  - init position array (pos_sim) to zeroes
  - for each day from 0 to holddays -1 :
        - get lagged long positions (long_sim_lag) and short positions (short_sim_lag)
        - update pos_sim array based on positions
  - cal sim returns (ret_sim) based on positions and market returns
- clean ret_sim
- if avg ret_sim is >= avg marketRet:
* increment numSampleAvgretBetterOrEqualObserved by 1 """
```
</details>

Results:
out of 10,000 random return sets, 1,166 have avg strat greater or equal to the observed avg return. So the null hypothesis can be rejected with only 88% probability. 
The shape of the returns distribution curve has something to do with the success of the strategy. 

---

<details open> 
  <summary>Code: T3 Randomizing the long and short entry dates</summary>

```python
# using same varibales of longs, shorts, cl, marketRet, lookback, holddays, and ret 
# using backshift, 

num_samples = 10000
numSampleAvgretBetterOrEqualObserved = 0

# simualte 
for _ in range(num_samples):
    # shuffle long and short signals to create a random permutation
    P = np.random.permutation(len(longs))

    # apply permutation to shuffle the signals
    longs_sim = longs[P]
    shorts_sim = shorts[P]

    # init position array with 0's
    pos_sim = np.zeros(len(cl), dtype=int)

    # compute positions based on shuffled singals over the holding period 
    for h in range(holddays):
        # get lagged long positions
        long_sim_lag = backshift(h, longs_sim)
        # clean
        long_sim_lag[np.isnan(long_sim_lag)] = False
        # convert to bool array
        long_sim_lag = long_sim_lag.astype(bool)

        # get lagged short positions
        short_sim_lag = backshift(h, shorts_sim)
        # clean 
        short_sim_lag[np.isnan(sort_sim_lag)] = False
        # convert to bool array
        short_sim_laf = short_sim_lag.astype(bool)
        
        # udate positions based on long and short signals 
        pos_sim[long_sim_lag] += 1
        pos_sim[short_sim_lag] -= 1 

    # calc simulated returns 
    ret_sim = backshift(1, pos_sim)*marketRet / holddays
    # clean rep nan w 0
    ret_sim[np.isnan(ret_sim)] = 0

    # chekc if avg sim return is greater than or equal to the average observed return 
    if np.mean(ret_sim) >= np.mean(ret):
        numSampleAvgretBetterOrEqualObserved += 1

# print num sim where avg return >= observed return 
```
</details>

Third hypothesis test: 
Result: no sample is above the avg observed return.

---

The fact that a null hypothesis is not unique and different null hypotheses can give rise to different estimates of statistical significance is one reason why many critics believe that hypothesis testing is flawed. 

What we actually want to know from the hypothesis test is the conditional probability that the null hypothesis is true given that we have observed the test stat $R:P(H_0 \mid R)$. But the procedure computes the conditional probability of obtaining a test stat $R$ given that the null hypothesis is true $P(R \mid H_0)$. Rarely is $P(H_0 \mid R) = P(R \mid H_0)$. 

Even though hypothesis testing and the rejection of the null hypothesis may not be a satisfactory way to estimate statistical significance the failure to reject a null hypothesis can inspire very interesting insights. 

### When not to backtest a strat

Some strategies are so obviously flawed that it would be a waste of time to even consider them. 

**Example 1:** a strat that has a backtest annualized return of 30% and a Sharpe ratio of 0.3, maximum drawdown duration of two years. 

Low Sharpe ratio with a long drawdown duration indicates that the strategy is not consistent. The high average return may be just a fluke, and it's not likey to repeat itself when we trade live. 

*Don't backtest high return but low sharpe ratio strategies*
*Don't bother testing strats with down duration longer than what investors can endure* 

**Example 2:** 
A long-only crude oil futures strategy returned 20% in 2007, sharpe ratio of 1.7. 
A quick check of the total return of holding the front-month crude oil futures in 2007 reveals that is was 47% with a sharpe ratio of 1.7. 

Trading strat is not in any way superior to a simple buy-hold strat. *choose the appropriate benchmark to measure a trading strat against*. The appropriate benchmark of a long-only strat is the return of a buy-and-hold position - the *information ratio* rather than the sharpe ratio. 

**Example 3:** A simple buy low sell high strategy picks the 10 lowest prices stocks at the beginning of the year and holds then for a year. The backtest return in 2001 is 388%. 
Was the strategy using survivorship bias free data? 
- if the database contains delisted stock the strat will most likely pick these for the portfolio, resulting in almost 100% loss. This would be realized return if we traded it back in 2001.

**Example 4** A neural new trading model that has about 100 nodes generates a backtest of Sharpe ratio 6
With 100 parameters we can certainly fit the model to any time series we want and obtain a high sharpe ratio. It will have little to no predictive power going forward due to data-snopping bias. 

**Example 5:** a HF E-mini sp500 futures trading strat has a backtest annula avg return of 200% and a sharpe ratio of 6. Avg. holding period is 50 seconds. 
Can we really backtest a HFT strat? Super dependent on market participants, “Heisenberg uncertainty principle” at work: The
act of placing or executing an order might alter the behavior of the other
market participants. So be very skeptical of a so-called backtest of a highfrequency strategy.

### Will a backtest be predictive of future returns? 
The predictive power of any backtest rests on the central assumption that the statistical properties of the price series are unchanging so that the trading rules that were profitable in the past will be profitable in the future.

Backtesting done prior or after regime shifts are quite worthless. 
